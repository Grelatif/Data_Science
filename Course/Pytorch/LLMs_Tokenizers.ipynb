{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEhhuL7MHO8IA2iVedmcQc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Grelatif/Data_Science/blob/main/LLMs_Tokenizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Exploration des Tokenizers Hugging Face\n",
        "\n",
        "# Installe la biblioth√®que Transformers\n",
        "!pip install -q transformers\n",
        "\n",
        "# Imports\n",
        "from transformers import (\n",
        "    BertTokenizer, RobertaTokenizer, GPT2Tokenizer,\n",
        "    DistilBertTokenizer, T5Tokenizer, XLNetTokenizer, BartTokenizer\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "fwQdDBxT6siT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sentences = [\n",
        "    \"I love Hugging Face and machine learning!\",\n",
        "    \"Transformers are amazing for NLP tasks.\",\n",
        "    \"Short sentence.\",\n",
        "    \"Another example to explore tokenizers.\",\n",
        "    \"How do we handle different languages?\"\n",
        "]"
      ],
      "metadata": {
        "id": "lQQcEOX56sfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "print(\"BERT Tokenizer\")\n",
        "for sentence in sentences:\n",
        "    tokens = tokenizer.tokenize(sentence)# tokens de la phrase de base\n",
        "    encoded = tokenizer(sentence)#encode la phrase en un dictionnaire contenant plusieurs informations (inputs_id, attention_mask, token_type_id, mais pas les tokens)\n",
        "    decoded = tokenizer.decode(encoded['input_ids'])\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Input IDs: {encoded['input_ids']}\")\n",
        "    print(f\"Attention Mask: {encoded.get('attention_mask', 'N/A')}\")\n",
        "    print(f\"Decoded: {decoded}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# token_type_id :\n",
        "# token_type_ids sont des identifiants de type de token qui sont utilis√©s pour indiquer √† BERT\n",
        "# quels tokens appartiennent √† quelle partie du texte, notamment dans les cas o√π tu as\n",
        "# deux segments de texte (par exemple, une question et sa r√©ponse).\n"
      ],
      "metadata": {
        "id": "ve0h0ecb6sdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(f\"Attention Mask: {encoded.get('attention_mask', 'N/A')}\")\n",
        "encoded[\"attention_mask\"]\n",
        "encoded.get('attention_mask', 'N/A')"
      ],
      "metadata": {
        "id": "I7eLpTfp6sap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RoBERTa Tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "print(\"RoBERTa Tokenizer\")\n",
        "for sentence in sentences:\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    encoded = tokenizer(sentence)\n",
        "    decoded = tokenizer.decode(encoded['input_ids'])\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Input IDs: {encoded['input_ids']}\")\n",
        "    print(f\"Attention Mask: {encoded.get('attention_mask', 'N/A')}\")\n",
        "    print(f\"Decoded: {decoded}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "VnxH_l9Q6sYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** GPT2: **\n",
        "\n",
        "GPT-2 est un mod√®le de langage auto-r√©gressif, ce qui signifie qu'il g√©n√®re du texte un token √† la fois. Il prend en entr√©e une s√©quence de tokens et g√©n√®re le prochain token de mani√®re s√©quentielle. Il n'a pas besoin de token de d√©but ([CLS]) ou de s√©parateur ([SEP]) parce qu'il g√©n√®re du texte dans une seule s√©quence continue.\n",
        "\n",
        "Contrairement √† BERT, qui est bidirectionnel et a √©t√© form√© avec des masques de tokens pour des t√¢ches comme la classification de texte ou la pr√©diction de masques, GPT-2 g√©n√®re simplement du texte √† partir d'une s√©quence d'entr√©e et a besoin de contextualiser chaque token en fonction des pr√©c√©dents.\n",
        "\n",
        " \"ƒ†\" repr√©sente un espace dans la tokenisation de GPT-2. C'est un token sp√©cial qui indique o√π un mot commence apr√®s un espace.\n",
        "\n"
      ],
      "metadata": {
        "id": "-hIKySpB7BiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-2 Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "print(\"GPT-2 Tokenizer\")\n",
        "for sentence in sentences:\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    encoded = tokenizer(sentence)\n",
        "    decoded = tokenizer.decode(encoded['input_ids'])\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Input IDs: {encoded['input_ids']}\")\n",
        "    print(f\"Attention Mask: {encoded.get('attention_mask', 'N/A')}\")\n",
        "    print(f\"Decoded: {decoded}\")\n",
        "    print(\"-\" * 50)\n",
        "    encoded\n"
      ],
      "metadata": {
        "id": "7w88YJ1O6sUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DistillBert Tokenizer**\n",
        "\n",
        "Tres similaire √† celui de BERT"
      ],
      "metadata": {
        "id": "kgOMNTcB7I40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DistilBERT Tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "print(\"DistilBERT Tokenizer\")\n",
        "for sentence in sentences:\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    encoded = tokenizer(sentence)\n",
        "    decoded = tokenizer.decode(encoded['input_ids'])\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Input IDs: {encoded['input_ids']}\")\n",
        "    print(f\"Attention Mask: {encoded.get('attention_mask', 'N/A')}\")\n",
        "    print(f\"Decoded: {decoded}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "J5jT6-JY6sSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** T5 Tokenizer**\n",
        "\n",
        "Le T5 tokenizer utilise une m√©thode de tokenisation appel√©e SentencePiece.\n",
        "\n",
        "SentencePiece est une approche qui d√©coupe le texte en sous-mots (similaire √† WordPiece), mais contrairement √† WordPiece, il n'a pas besoin d'un vocabulaire pr√©existant. SentencePiece apprend son vocabulaire directement √† partir des donn√©es et peut √©galement g√©rer des unit√©s de caract√®res.\n",
        "\n",
        "SentencePiece fonctionne sur l'id√©e que les mots ou les caract√®res peuvent √™tre des unit√©s atomiques (par exemple, une unit√© pourrait √™tre un caract√®re, une racine de mot ou un mot entier).\n",
        "\n",
        "Exemple : Le mot \"unhappiness\" pourrait √™tre d√©coup√© en [\"‚ñÅun\", \"happiness\"], o√π \"‚ñÅ\" repr√©sente un espace dans le vocabulaire de SentencePiece.\n",
        "\n"
      ],
      "metadata": {
        "id": "6P8rvk8M7NuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# T5 Tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "\n",
        "print(\"T5 Tokenizer\")\n",
        "for sentence in sentences:\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    encoded = tokenizer(sentence)\n",
        "    decoded = tokenizer.decode(encoded['input_ids'])\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Input IDs: {encoded['input_ids']}\")\n",
        "    print(f\"Attention Mask: {encoded.get('attention_mask', 'N/A')}\")\n",
        "    print(f\"Decoded: {decoded}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "6l75Boh86sP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** XLnet **\n",
        "\n",
        "XLNet combine les caract√©ristiques des mod√®les autoregressifs (comme GPT) et des mod√®les bidirectionnels (comme BERT).\n",
        "\n",
        "BERT apprend √† pr√©dire des tokens masqu√©s dans une s√©quence en utilisant un apprentissage bidirectionnel, ce qui signifie qu'il regarde √† la fois les tokens √† gauche et √† droite du token masqu√©.\n",
        "\n",
        "XLNet n'utilise pas de masquage traditionnel comme BERT. Au lieu de cela, il utilise une approche de permutation autoregressive, o√π il apprend √† pr√©dire chaque token en fonction des autres tokens dans une permutation al√©atoire de la s√©quence.\n",
        "\n",
        "Cela signifie qu'XLNet permet au mod√®le de capturer des relations contextuelles √† la fois de gauche √† droite et de droite √† gauche sans avoir besoin d'un token de type [CLS] ou [SEP], comme dans BERT."
      ],
      "metadata": {
        "id": "m2Ac-K9C7VGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZYA8GchL7Yo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XLNet Tokenizer\n",
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
        "\n",
        "print(\"XLNet Tokenizer\")\n",
        "for sentence in sentences:\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    encoded = tokenizer(sentence)\n",
        "    decoded = tokenizer.decode(encoded['input_ids'])\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Input IDs: {encoded['input_ids']}\")\n",
        "    print(f\"Attention Mask: {encoded.get('attention_mask', 'N/A')}\")\n",
        "    print(f\"Decoded: {decoded}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "G1xGbm6n6sNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bart**\n",
        "\n",
        "Tokenisation avec BPE : BART utilise un vocabulaire bas√© sur BPE, ce qui permet de d√©couper les mots en sous-mots.\n",
        "\n",
        "BPE est une m√©thode similaire √† WordPiece et SentencePiece dans le sens o√π elle d√©coupe des mots en sous-mots pour les rendre plus manipulables par le mod√®le, mais l'algorithme d'apprentissage est l√©g√®rement diff√©rent.\n",
        "\n",
        "BPE cherche √† maximiser la fr√©quence des paires de caract√®res dans un texte, et en combinant ces paires, le vocabulaire se construit de mani√®re dynamique.\n"
      ],
      "metadata": {
        "id": "GT0_rEmh7cqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  BART Tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
        "\n",
        "print(\"BART Tokenizer\")\n",
        "for sentence in sentences:\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    encoded = tokenizer(sentence)\n",
        "    decoded = tokenizer.decode(encoded['input_ids'])\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Input IDs: {encoded['input_ids']}\")\n",
        "    print(f\"Attention Mask: {encoded.get('attention_mask', 'N/A')}\")\n",
        "    print(f\"Decoded: {decoded}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "bKivZY256sLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch Encoding avec Padding et Troncature\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "batch = tokenizer.batch_encode_plus(\n",
        "    sentences,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(\"Batch Encoding\")\n",
        "print(batch)\n"
      ],
      "metadata": {
        "id": "_muTtmt56sIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparaison du nombre de tokens pour chaque tokenizer dans une liste de phrases\n",
        "tokenizers = {\n",
        "    \"BERT\": BertTokenizer.from_pretrained(\"bert-base-uncased\"),\n",
        "    \"RoBERTa\": RobertaTokenizer.from_pretrained(\"roberta-base\"),\n",
        "    \"GPT-2\": GPT2Tokenizer.from_pretrained(\"gpt2\"),\n",
        "    \"T5\": T5Tokenizer.from_pretrained(\"t5-small\"),\n",
        "}\n",
        "\n",
        "print(\"üîç Comparaison des tokens pour chaque tokenizer:\")\n",
        "for name, tokenizer in tokenizers.items():\n",
        "    for sentence in sentences:\n",
        "        tokens = tokenizer.tokenize(sentence)\n",
        "        print(f\"{name} -> Sentence: '{sentence}' -> {len(tokens)} tokens\")\n",
        "        print(f\"{name} -> tokens: {tokens}\")\n",
        "\n",
        "\n",
        "# QUite a lot of differences, and usecases.\n"
      ],
      "metadata": {
        "id": "0N9C7D7W6sGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # let's wrap up all the info we saw\n",
        "\n",
        "# Mod√®le\t    Tokenizer\t          M√©thode\t    Tokens sp√©ciaux (exemples)\t    Type de mod√®le\t          Utilisation typique\n",
        "# BERT\t      BertTokenizer\t      WordPiece\t    [CLS], [SEP], [PAD], [MASK]\t  Encodeur\t                Classification, QA, NER\n",
        "# DistilBERT\tDistilBertTokenizer\tWordPiece\t    [CLS], [SEP], [PAD], [MASK]\t  Encodeur                  (light BERT)\tIdem BERT, + rapide\n",
        "# RoBERTa\t    RobertaTokenizer\t  BPE\t          <s>, </s>, <pad>, <mask>\t    Encodeur\t                Idem BERT, + robuste\n",
        "# XLNet\t      XLNetTokenizer\t    WordPiece\t    [CLS], [SEP], [PAD], [MASK]\t  Permut√© + autoregressif \tQA, texte long, perf. √©lev√©e\n",
        "# GPT-2\t      GPT2Tokenizer\t      BPE\tAucun     [CLS]/[SEP], pas de padding\t  D√©codeur autoregressif\t  G√©n√©ration de texte\n",
        "# BART\t      BartTokenizer\t      BPE\t          <s>, </s>, <pad>, <mask>\t    Encodeur-d√©codeur\t        R√©sum√©, traduction, g√©n√©ration\n",
        "# T5\t        T5Tokenizer\t        SentencePiece\t<pad>, </s>, <unk>\t          Encodeur-d√©codeur\t        T√¢ches text-to-text\n",
        "\n"
      ],
      "metadata": {
        "id": "N9T8xMR76sC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fJi7hGqg6sAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KJuxT5P-6r-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8p2Rf3Wp6r77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wr05fe3u6r5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HaAq1KPn6r2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ppHVFtc6qHl"
      },
      "outputs": [],
      "source": []
    }
  ]
}
