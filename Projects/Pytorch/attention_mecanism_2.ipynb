{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7mV7D+8etkcpGy6Q0oD+0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElMartinez31/Data_Science/blob/main/Projects/Pytorch/attention_mecanism_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exemple from https://github.com/priyammaz/PyTorch-Adventures/blob/main/PyTorch%20for%20Transformers/Attention%20Mechanisms/Attention/attention_mechanism.ipynb"
      ],
      "metadata": {
        "id": "Zr6wElmZ85Pb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbXIuIFj80xX",
        "outputId": "3d8e0ab1-73fb-4e5a-de09-cdbc512601b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Input is: torch.Size([4, 3, 128])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "### Lets Define some Random Data ###\n",
        "batch_size = 4\n",
        "sequence_length = 3 # 3 rows\n",
        "embed_dim = 128 # 128 cols\n",
        "\n",
        "x = torch.randn(batch_size, sequence_length, embed_dim)\n",
        "print(\"Shape of Input is:\", x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmy6jKdT819a",
        "outputId": "66f9ac65-2527-4f9f-e9cc-48e01791ec28"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 9.9590e-02, -1.1695e+00,  1.7312e+00, -7.7031e-01,  5.7080e-01,\n",
            "         -1.7259e+00,  1.6775e+00,  4.8589e-01, -1.0152e+00, -4.9480e-01,\n",
            "         -4.5230e-02,  1.3933e+00, -2.9162e-01,  8.8782e-02, -5.2840e-01,\n",
            "         -1.5177e+00, -7.8065e-01,  2.5851e+00, -4.6460e-01,  7.0644e-01,\n",
            "          1.1009e+00, -5.8422e-01,  9.7801e-01,  3.1384e-02,  1.7573e+00,\n",
            "         -1.1259e+00,  2.5316e-02,  1.1250e+00,  3.6496e-01,  7.4484e-01,\n",
            "          1.1244e+00,  9.4252e-01, -2.8845e-01, -7.2158e-01, -5.2712e-01,\n",
            "          5.0121e-01,  5.8927e-01, -7.6326e-01, -6.1938e-02, -4.3369e-01,\n",
            "         -1.4464e+00,  3.3717e-01, -1.9274e+00, -7.3527e-01, -2.3908e-01,\n",
            "         -9.4332e-02,  8.1856e-01,  1.0238e-01,  3.7208e-01, -4.0831e-01,\n",
            "          1.0534e+00, -7.8330e-01, -3.5953e-01, -1.0684e-01, -8.2355e-02,\n",
            "         -6.8751e-03, -4.6203e-01, -1.7463e+00,  8.9102e-01, -1.7375e+00,\n",
            "          3.2351e-03,  2.1015e+00, -6.5562e-01,  3.7066e-01,  3.9552e-01,\n",
            "         -1.1318e-02, -6.8300e-01, -1.5589e+00, -1.9112e-01,  5.5755e-01,\n",
            "         -3.2824e-02,  1.3968e+00,  5.9840e-01,  2.9602e-01, -2.0608e-01,\n",
            "         -1.3283e+00,  2.5324e-02,  2.0213e+00, -1.1031e-01,  5.0143e-01,\n",
            "          6.2251e-01, -3.2787e-01,  1.0225e+00,  2.8695e+00, -5.4546e-01,\n",
            "         -6.6527e-01, -4.9788e-01,  9.9639e-01, -6.0992e-01,  3.3157e-01,\n",
            "         -7.5646e-01, -4.4704e-01,  5.0769e-01,  1.2123e+00,  1.5720e+00,\n",
            "          3.5181e-01, -4.3530e-01,  1.6489e+00,  6.9000e-01, -9.1715e-01,\n",
            "         -1.3142e+00, -6.0961e-01, -1.3909e+00, -1.6842e+00, -1.1845e-01,\n",
            "          1.0819e-01, -1.2988e+00,  1.6770e+00,  1.1715e+00,  2.0127e+00,\n",
            "         -1.8685e-01, -1.7254e+00, -4.8761e-02, -3.5926e-01,  2.5749e+00,\n",
            "         -9.7237e-03,  5.2586e-01, -4.1040e-01,  2.7247e+00, -3.4507e-03,\n",
            "         -1.2220e-01,  6.0289e-01, -1.8686e+00, -7.9494e-01,  9.5427e-01,\n",
            "          1.0851e+00,  1.7771e-01,  3.1605e-02],\n",
            "        [ 7.9965e-01,  1.5731e+00,  1.1013e+00, -6.4857e-01,  1.1145e+00,\n",
            "          1.5818e+00,  1.5028e-01,  7.9862e-01,  5.0471e-01,  1.9208e-01,\n",
            "          2.9033e+00, -1.1290e+00, -1.5057e+00, -1.1914e+00, -1.8192e-01,\n",
            "          1.6101e+00,  2.8478e-01, -2.0427e+00, -7.2502e-01, -1.3430e+00,\n",
            "         -1.1229e+00, -1.3496e+00, -1.1451e+00, -5.5155e-01, -2.2875e+00,\n",
            "          1.2599e+00,  1.7492e-01,  1.5641e+00, -1.1652e-01, -3.5980e-01,\n",
            "         -5.0025e-01,  9.4153e-01, -2.1224e-01, -1.1996e+00,  5.1220e-01,\n",
            "          1.6240e+00, -1.3749e-01,  8.5704e-02,  9.9024e-01,  1.5515e+00,\n",
            "         -9.8577e-01,  5.4138e-01,  1.0169e+00,  7.0942e-01, -1.5673e+00,\n",
            "         -8.8814e-01, -1.5552e+00,  1.1152e+00, -1.4187e+00, -2.4484e-01,\n",
            "         -7.4238e-02, -1.4953e+00,  2.4086e-01, -4.4660e-01,  5.4977e-01,\n",
            "         -1.2868e+00,  1.0586e+00,  4.0343e-01, -1.7754e+00,  4.4096e-01,\n",
            "          1.0089e-02, -4.5473e-01,  3.3837e-01,  8.4836e-01, -1.9119e+00,\n",
            "          7.0648e-02, -2.8612e-01, -7.4078e-02,  4.9239e-01,  8.6372e-01,\n",
            "         -7.7317e-01,  4.7920e-01,  2.7182e-01, -1.6291e-01, -7.3281e-01,\n",
            "          4.1014e-01, -1.7766e+00, -1.6545e+00, -5.1163e-01,  1.9443e+00,\n",
            "         -4.7366e-02,  9.6613e-01,  7.9564e-01, -4.5796e-01, -1.0954e+00,\n",
            "          1.7893e+00,  1.3325e-01, -1.4754e+00,  1.6966e+00,  5.5983e-01,\n",
            "         -1.0043e+00,  5.4924e-02,  8.8277e-01, -1.9163e+00, -5.5397e-01,\n",
            "          4.4229e-02, -8.6705e-01,  8.8867e-01,  6.6674e-01, -1.6613e+00,\n",
            "         -1.8835e+00,  1.0005e-01,  1.0157e+00, -1.0373e+00,  1.5039e-01,\n",
            "         -4.8855e-01, -3.1252e-01, -1.3784e-01, -1.4858e+00,  4.8513e-01,\n",
            "          1.2977e+00, -3.4197e+00,  2.2413e-01, -5.9426e-01, -1.2695e-01,\n",
            "          6.9192e-01, -1.1607e+00,  7.7906e-01,  1.1167e+00, -6.8855e-01,\n",
            "          2.6573e-02, -6.1813e-01,  1.2317e-01,  8.9328e-01, -1.9724e+00,\n",
            "         -7.8756e-01, -1.2495e+00, -9.1019e-01],\n",
            "        [-1.1512e+00,  1.1806e+00,  7.1625e-01, -3.8587e-01,  9.6092e-01,\n",
            "         -3.1561e-01,  1.9223e-01, -4.3585e-01, -2.5210e+00, -1.2367e+00,\n",
            "         -1.2907e+00,  4.9692e-01,  1.6092e+00,  8.0941e-01, -6.3979e-01,\n",
            "         -9.8378e-01,  9.0570e-01,  1.0553e+00,  3.3595e-01, -3.8160e-01,\n",
            "          1.2269e+00, -6.1466e-01,  2.2809e+00,  1.1553e+00, -1.3307e+00,\n",
            "          3.9104e-01,  1.9061e+00,  1.9635e+00,  1.5875e+00, -1.1427e+00,\n",
            "          4.4425e-01, -1.3373e+00,  4.3546e-01, -3.6561e-01,  1.1823e-01,\n",
            "         -1.1138e+00,  1.7528e+00, -5.5050e-01, -3.4588e-02, -8.3696e-01,\n",
            "         -4.7544e-01, -6.8073e-01, -6.3508e-01,  7.5603e-01,  6.5721e-01,\n",
            "          3.3419e-01,  6.1965e-01, -1.4590e+00, -3.0551e+00,  8.3868e-01,\n",
            "          5.5089e-01,  5.8684e-01,  3.8202e-01,  1.6070e+00,  2.4736e-01,\n",
            "         -1.1496e+00,  1.2787e+00,  6.2386e-01, -9.3402e-01, -1.3274e+00,\n",
            "          1.8710e-01, -1.2257e+00, -9.0534e-01, -5.6736e-01,  6.3782e-02,\n",
            "          3.6326e-01, -1.1959e+00, -3.3439e-01,  7.6821e-01, -8.0905e-01,\n",
            "         -1.6277e+00, -7.8107e-01, -1.6134e-01,  3.9169e-01,  3.7144e-01,\n",
            "          7.0770e-01, -1.7465e+00,  9.0443e-01,  1.3492e+00, -2.0080e+00,\n",
            "          6.7809e-01, -5.0459e-02,  4.1690e-01, -2.1676e+00,  2.3674e+00,\n",
            "          2.5933e-01,  5.5330e-02,  1.0989e+00,  1.2546e+00, -8.9338e-01,\n",
            "         -7.4036e-01,  5.6986e-01, -3.8303e-01, -1.3047e-01, -2.0540e+00,\n",
            "         -1.2293e+00,  1.1835e+00, -1.3295e-01, -3.0848e-02, -2.8883e-01,\n",
            "         -7.8059e-02,  4.7906e-02, -8.2616e-01,  2.1558e-01,  1.0030e-01,\n",
            "          1.1885e+00,  7.2820e-01,  7.1283e-01, -1.0557e+00,  1.0316e+00,\n",
            "         -5.6771e-02,  1.0125e+00,  2.7746e+00, -3.9671e-01,  7.8086e-01,\n",
            "         -9.1496e-01, -3.7102e-01,  2.5264e+00,  1.4477e+00, -9.1866e-02,\n",
            "          1.4485e+00,  7.8210e-01,  1.8045e+00, -5.4155e-01, -9.0356e-02,\n",
            "         -5.9746e-01, -6.1376e-01,  1.6729e+00]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1\n",
        "\n",
        "Compute XXt\n",
        " which will provide the similarity score between every pair of vectors in\n",
        "This will be contained inside a batch x sequence_length x sequence_length matrix"
      ],
      "metadata": {
        "id": "WEzVwq8d9rYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# similarity matrix\n",
        "res = x @ x.transpose(1,2)# 1,2 car le tensor est multidim (4,3,128)\n",
        "res\n",
        "\n",
        "\n",
        "# res.shape # torch.Size([4, 3, 3])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFfRbv8k817Y",
        "outputId": "2c92c6e5-8493-47f3-8842-cd9769d36f81"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[134.4914, -19.8600,  -0.8542],\n",
              "         [-19.8600, 149.9494, -10.3931],\n",
              "         [ -0.8542, -10.3931, 151.1074]],\n",
              "\n",
              "        [[154.2502,  -2.1142,   6.0908],\n",
              "         [ -2.1142, 128.8848,  13.5910],\n",
              "         [  6.0908,  13.5910, 125.6071]],\n",
              "\n",
              "        [[142.6484,   4.1989,   8.5398],\n",
              "         [  4.1989, 153.2547,  -1.2877],\n",
              "         [  8.5398,  -1.2877, 121.1813]],\n",
              "\n",
              "        [[118.1097,  -5.5167,  20.7462],\n",
              "         [ -5.5167, 131.1252,  -2.4520],\n",
              "         [ 20.7462,  -2.4520, 118.7423]]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "res.var()#tensor(384.0310) very large var, that we want to reduce (large the embedding dim, larger the var)\n",
        "# That's why we divide by sqrt(dim_embedding)\n",
        "\n",
        "\n",
        "res_norm  = res/np.sqrt(embed_dim)\n",
        "res.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1nj42m0813K",
        "outputId": "b5e83064-561b-4c2d-c5cc-02e7ed739d99"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4292.4810)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res_norm.var() #tensor(3.0002)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT1JMHKW810w",
        "outputId": "766fb6ad-75cc-429d-e4b9-e20795de8e3b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(33.5350)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res_norm # 4 batches of 3x3 matrix of similarity (but not on its final form yet)\n",
        "# we want the rows to add to 1 (proba) ==> softmax\n",
        "\n",
        "attention_matrix = res_norm.softmax(-1) # (or softmax(2) which is the very same)\n",
        "\n",
        "\n",
        "attention_matrix"
      ],
      "metadata": {
        "id": "xpWe1aGZ81yo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5fd3286-cef6-4240-9f0a-1c0ecbb33bb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[9.9931e-01, 3.6891e-04, 3.1834e-04],\n",
              "         [1.3166e-05, 9.9996e-01, 3.1858e-05],\n",
              "         [1.3423e-06, 3.7640e-06, 9.9999e-01]],\n",
              "\n",
              "        [[9.9996e-01, 8.5540e-06, 2.6807e-05],\n",
              "         [8.7954e-07, 1.0000e+00, 2.2784e-06],\n",
              "         [2.1342e-05, 1.7641e-05, 9.9996e-01]],\n",
              "\n",
              "        [[9.9906e-01, 1.8538e-04, 7.5534e-04],\n",
              "         [3.4885e-06, 9.9998e-01, 1.4854e-05],\n",
              "         [1.4729e-04, 1.5392e-04, 9.9970e-01]],\n",
              "\n",
              "        [[9.9961e-01, 1.6618e-05, 3.7075e-04],\n",
              "         [7.2044e-07, 1.0000e+00, 1.7631e-06],\n",
              "         [1.4680e-05, 1.6102e-06, 9.9998e-01]]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_matrix\n",
        "\n",
        "one = attention_matrix[0]\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize = (10,8))\n",
        "sns.heatmap(one, annot = True, fmt =\"0.2g\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "8y9FbZ6381wI",
        "outputId": "1df8f47c-ae55-4b45-cedb-0edfb1266717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summed_attention_mat  = attention_matrix.sum(axis=-1)\n",
        "summed_attention_mat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKqOE5CUFq8B",
        "outputId": "8836f129-e3c7-49cf-cd9a-213907a64eaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 1.0000, 1.0000],\n",
              "        [1.0000, 1.0000, 1.0000],\n",
              "        [1.0000, 1.0000, 1.0000],\n",
              "        [1.0000, 1.0000, 1.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute output\n",
        "attention_matrix.shape #torch.Size([4, 3, 3])\n",
        "x.shape #torch.Size([4, 3, 128])\n",
        "\n",
        "context_vectors  = attention_matrix @ x #should be (4,3,128)\n",
        "context_vectors.shape#torch.Size([4, 3, 128])\n",
        "context_vectors\n",
        "# And this is attention, one of the most important technic used in AI\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3DcADpf81t1",
        "outputId": "7849f6ff-5785-4ea7-c10c-db039aaeca40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.5545,  0.8880, -0.2507,  ...,  1.0814, -0.0484, -0.8474],\n",
              "         [ 1.0513, -0.5523, -0.5312,  ...,  1.8571, -0.2668,  0.2580],\n",
              "         [ 0.1441, -1.8767, -0.8102,  ..., -0.1960, -0.0090,  0.1419]],\n",
              "\n",
              "        [[ 1.0454, -1.3301,  0.5929,  ...,  1.1602, -0.9664, -1.3006],\n",
              "         [ 1.8356, -0.2614, -2.4894,  ...,  0.4530, -1.9985, -0.3762],\n",
              "         [ 1.1905, -1.4044, -0.0158,  ...,  0.7896,  3.0875, -0.0199]],\n",
              "\n",
              "        [[-1.1028,  0.8221, -0.4931,  ...,  0.8397, -0.3172,  0.2348],\n",
              "         [ 0.0635,  0.3022,  0.9358,  ..., -1.0514,  1.1721,  0.3927],\n",
              "         [ 0.5900, -0.2650,  0.9203,  ..., -0.4701,  1.3915,  0.5546]],\n",
              "\n",
              "        [[ 0.6323,  1.0205,  0.1019,  ...,  0.5889,  0.4760,  0.7920],\n",
              "         [ 0.8198, -0.5914, -0.8259,  ..., -0.7429, -1.8682,  1.2592],\n",
              "         [-0.6215,  0.3486, -0.7160,  ...,  1.1727, -0.8769, -0.9450]]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reminder:\n",
        "# if we do have a linear layer like that:\n",
        "fc1 = nn.Linear(10,20) # 10in, 20 out\n",
        "\n",
        "# and we also have a batch like that (batch_size, seq, embed_dim)\n",
        "rand = torch.rand(4,6,10) #4batches of 6 tokenns which are embedded in a 10 dim\n",
        "\n",
        "#pytorch directly multiplies the number of batches and the number of tokens for us, keeping the last dim (10 here) as the input dim\n"
      ],
      "metadata": {
        "id": "TENO0e80F9if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# building attention (single head)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, embedding_dim):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embedding_dim\n",
        "    self.query = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.key = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.value = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    q = self.query(x)\n",
        "    k = self.key(x)\n",
        "    v =  self.value(x)\n",
        "    context = (q @ k.transpose(1,2))/(self.embed_dim ** 0.5) # (batch_size, sequence_length, sequence_length)\n",
        "    attention = context.softmax(axis = -1) @ v # normalise ces scores pour qu'ils somment à 1 par ligne (probabilités d'attention).\n",
        "    return attention\n",
        "\n",
        "rand = torch.rand(4,6,128)\n",
        "\n",
        "att = Attention(128)\n"
      ],
      "metadata": {
        "id": "4C77pzam81rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "att\n",
        "#Attention(\n",
        "#   (query): Linear(in_features=128, out_features=128, bias=True)\n",
        "#   (key): Linear(in_features=128, out_features=128, bias=True)\n",
        "#   (value): Linear(in_features=128, out_features=128, bias=True)\n",
        "# )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqUItiDpnJJn",
        "outputId": "3e1b8858-8165-4fc9-aa74-64ea1c4aea1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Attention(\n",
              "  (query): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (key): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (value): Linear(in_features=128, out_features=128, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now for the multi head attention we can check the reference notebook,\n",
        "# but as just a reminder, for the multihead we split our embedding_dim by the number of heads\n",
        "# so if our embedding_dim = 9 and we want 3 head, we will have a head_dim of 9/3\n",
        "# meaning that Q, K and V will be (Seq_lenght x head_dim) => context will still be seq x seq but output will be seq x head_dim\n",
        "# We do this for every head of attention (so 3 times, ) and the idea is that every head will capture a diffrent pattern.\n",
        "# Then we concatenate the 3 outputs to get the final output.\n",
        "\n",
        "\n",
        "# Pour gérer dynamiquempent ces différentes tetes et leurs projections (Q,K et V), nous allons utiliser 2 conteneurs pytorch:\n",
        "\n",
        "# nn.ModuleList :\n",
        "# Une liste spéciale PyTorch pour stocker des modules (comme des couches nn.Linear).\n",
        "# Utilité : Permet à PyTorch de suivre les paramètres des modules pour l'apprentissage (contrairement à une liste Python standard).\n",
        "# Exemple ici : Stocke les projections Q/K/V pour chaque tête.\n",
        "# C’est une sous-classe de nn.Module qui sert à stocker une liste de modules PyTorch (comme nn.Linear, nn.Conv2d, etc.) tout en les enregistrant correctement dans le graphe de calcul.\n",
        "# Contrairement à une simple liste Python (list), nn.ModuleList :\n",
        "# enregistre les sous-modules dans le modèle principal (self)\n",
        "# les prend en compte automatiquement lors de l’appel de .to(), .cuda(), .eval(), .state_dict(), etc.\n",
        "# nn.Sequential applique les modules enchaînés automatiquement dans l’ordre.\n",
        "# nn.ModuleList ne fait rien automatiquement. Tu dois itérer toi-même dans forward().\n",
        "# Avec un nn.ModuleList, tu accèdes aux éléments via des indices\n",
        "\n",
        "# nn.ModuleDict :\n",
        "# Un dictionnaire spécial PyTorch pour stocker des modules avec des clés explicites.\n",
        "# Utilité : Rend le code plus lisible en accédant aux modules par nom (ex: head[\"Q\"]).\n",
        "# Exemple ici : Stocke séparément les projections Q, K, V pour une tête.\n",
        "# C’est une collection de sous-modules PyTorch stockés dans un dictionnaire (clés = noms, valeurs = modules).\n",
        "# Comme nn.ModuleList, mais au lieu d’une liste ordonnée, c’est une structure clé → module, donc plus flexible pour certains designs.\n",
        "# Avec un ModuleDict, tu peux accéder aux couches par leurs clés (\"Q\", \"K\", \"V\")\n",
        "\n",
        "# let's code a multihead class\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embedding_dimension, num_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        ### Make sure Embedding Dimension is Divisible by Num Heads ###\n",
        "        assert embedding_dimension % num_heads == 0, f\"Make sure your embed_dim {embedding_dimension} is divisible by the number of heads {num_heads}\"\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        ### Compute Head Dimension ###\n",
        "        self.head_dim = self.embed_dim // self.num_heads\n",
        "\n",
        "\n",
        "        ### Create a List of Lists which has all our Q,K,V projections for each head ###\n",
        "        self.multihead_qkv = nn.ModuleList()\n",
        "\n",
        "        ### For head Head create the QKV ###\n",
        "        for head in range(self.num_heads):\n",
        "\n",
        "            ### Create a dictionary of the 3 projection  layers we need ###\n",
        "            qkv_proj = nn.ModuleDict(\n",
        "                [\n",
        "                    [\"Q\", nn.Linear(self.embed_dim, self.head_dim)],\n",
        "                    [\"K\", nn.Linear(self.embed_dim, self.head_dim)],\n",
        "                    [\"V\", nn.Linear(self.embed_dim, self.head_dim)],\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            ### Store Dictionary in List ###\n",
        "            self.multihead_qkv.append(qkv_proj)\n",
        "\n",
        "        ### Create final Projection layer, it will be applied to the concatenated heads will have shape embed_dim again ###\n",
        "        self.head_mesh = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        ### Create a list ot store each heads output ###\n",
        "        head_outs = []\n",
        "\n",
        "        ### Loop Through Each head of Attention ###\n",
        "        for head in self.multihead_qkv:\n",
        "\n",
        "            ### Access layers like a dictionary (ModuleDict) ###\n",
        "            ### q,k,v will be (Batch x Seq len x head_dim)\n",
        "            q = head[\"Q\"](x)\n",
        "            k = head[\"K\"](x)\n",
        "            v = head[\"V\"](x)\n",
        "\n",
        "            ### Now do the same Attention computation as before! ###\n",
        "            similarity = (q @ k.transpose(1,2)) / (self.head_dim ** 0.5)\n",
        "            attention  = similarity.softmax(axis=-1)\n",
        "            output = attention @ v\n",
        "\n",
        "            ### Store this output in the head_outs ###\n",
        "            head_outs.append(output)\n",
        "\n",
        "        ### head_outs has num_heads tensors, each with the compressed embedding dimension of head_dim ###\n",
        "        ### We can concatenate them all back together along the embedding dimension just like we did in the image above ###\n",
        "        head_outs = torch.cat(head_outs, dim=-1) #(9,9)\n",
        "\n",
        "        ### head_outs will have the same shape now as our input x! ###\n",
        "        if head_outs.shape != x.shape:\n",
        "            raise Exception(\"Something has gone wrong in the attention computation\")\n",
        "\n",
        "        ### Now each head was computed independently, we need them to get to know each other, so pass our head_outs through final projection ###\n",
        "        output = self.head_mesh(head_outs)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "embed_dim = 9\n",
        "num_heads = 3\n",
        "seq_len = 8\n",
        "mha = MultiHeadAttention(embed_dim, num_heads)\n",
        "\n",
        "### Create a random tensor in the shape (Batch x Seq Len x Embed Dim) ###\n",
        "rand = torch.randn(3,seq_len,embed_dim)\n",
        "\n",
        "### Pass through MHA ###\n",
        "output = mha(rand)"
      ],
      "metadata": {
        "id": "Ck22ZPE_81pH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5-Uh2Y52cEN",
        "outputId": "41f5de41-dc8c-4e67-ee6d-82f2ecec1456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1.5035e-02, -5.3246e-01, -4.7115e-03,  5.5875e-01,  2.9850e-01,\n",
              "           2.2946e-02, -1.8906e-01,  4.6029e-01,  1.5554e-01],\n",
              "         [ 5.0744e-02, -5.0803e-01, -7.1455e-02,  5.0784e-01,  3.0439e-01,\n",
              "           3.3083e-02, -1.7280e-01,  4.6787e-01,  2.1325e-01],\n",
              "         [-9.1646e-02, -5.3901e-01,  8.3869e-02,  5.3256e-01,  3.6911e-01,\n",
              "           7.2691e-02, -2.2168e-01,  4.4651e-01,  1.6101e-01],\n",
              "         [-2.0774e-03, -4.9547e-01, -5.6488e-03,  4.7247e-01,  3.6710e-01,\n",
              "           1.1976e-01, -1.5393e-01,  5.0375e-01,  1.9772e-01],\n",
              "         [-2.1032e-02, -5.2656e-01,  4.9132e-02,  5.1301e-01,  3.5065e-01,\n",
              "           8.5316e-02, -1.8157e-01,  4.8411e-01,  1.8964e-01],\n",
              "         [ 1.1451e-03, -5.1146e-01, -4.6406e-02,  4.7722e-01,  3.4486e-01,\n",
              "           8.1129e-02, -1.7450e-01,  4.6755e-01,  2.2369e-01],\n",
              "         [-4.8530e-02, -5.7413e-01,  1.0848e-01,  6.0945e-01,  3.0739e-01,\n",
              "          -8.5592e-05, -2.0932e-01,  4.5267e-01,  1.3236e-01],\n",
              "         [-5.0189e-02, -5.4101e-01,  5.3173e-02,  5.2429e-01,  3.5845e-01,\n",
              "           8.0498e-02, -1.8800e-01,  4.7536e-01,  1.6402e-01]],\n",
              "\n",
              "        [[-1.4915e-01, -7.7489e-01,  2.0022e-01,  6.2009e-01,  4.0200e-01,\n",
              "          -3.1233e-03,  9.9072e-02,  6.4579e-01, -1.0777e-01],\n",
              "         [ 1.7680e-02, -7.2511e-01,  4.0656e-02,  5.3057e-01,  2.8514e-01,\n",
              "           1.1522e-02,  1.3543e-01,  6.2973e-01,  2.7664e-02],\n",
              "         [-7.4610e-02, -8.1667e-01,  1.7132e-01,  6.6111e-01,  3.1835e-01,\n",
              "          -1.2518e-02,  6.0713e-02,  5.9712e-01,  5.4388e-03],\n",
              "         [-2.2734e-01, -9.3118e-01,  3.1841e-01,  7.7510e-01,  3.9115e-01,\n",
              "          -6.0863e-02, -8.6848e-03,  5.7364e-01, -4.0106e-02],\n",
              "         [-9.4722e-02, -7.7338e-01,  1.0901e-01,  5.6794e-01,  3.8912e-01,\n",
              "           1.7479e-02,  1.4401e-01,  6.6757e-01, -5.7576e-02],\n",
              "         [-9.3931e-02, -8.2685e-01,  1.4015e-01,  6.3766e-01,  3.4452e-01,\n",
              "          -4.1247e-02,  1.1718e-01,  6.3398e-01, -4.3900e-02],\n",
              "         [-3.5935e-02, -7.7801e-01,  1.2924e-01,  6.3256e-01,  3.2664e-01,\n",
              "          -2.6751e-02,  1.0639e-01,  6.4534e-01, -4.2269e-02],\n",
              "         [-4.3663e-02, -7.8839e-01,  1.0483e-01,  5.8872e-01,  2.9774e-01,\n",
              "          -8.4099e-03,  9.8098e-02,  6.0436e-01,  2.5170e-02]],\n",
              "\n",
              "        [[ 8.4156e-02, -6.0868e-01, -1.2563e-01,  4.2406e-01,  3.9022e-01,\n",
              "           9.7646e-02, -1.1396e-02,  6.5157e-01,  7.1923e-02],\n",
              "         [ 5.1683e-02, -6.0537e-01, -1.1640e-01,  4.6641e-01,  3.7595e-01,\n",
              "           4.2519e-02,  4.4374e-03,  6.5218e-01, -8.3340e-03],\n",
              "         [-4.6652e-02, -4.5618e-01, -9.0635e-03,  4.3973e-01,  2.8431e-01,\n",
              "           1.1003e-01, -7.5111e-02,  4.9885e-01, -9.7015e-02],\n",
              "         [ 6.9695e-02, -5.2144e-01, -8.5454e-02,  3.6938e-01,  4.1800e-01,\n",
              "           2.0093e-01, -5.8975e-02,  6.3849e-01,  2.6967e-02],\n",
              "         [ 8.5855e-02, -5.9280e-01, -1.1909e-01,  4.1642e-01,  4.1304e-01,\n",
              "           1.2435e-01, -2.9574e-02,  6.7139e-01,  1.6975e-02],\n",
              "         [ 1.9469e-02, -5.3748e-01, -7.9608e-02,  4.6345e-01,  3.5359e-01,\n",
              "           9.1018e-02, -5.2309e-03,  6.2433e-01, -8.5002e-02],\n",
              "         [-8.0630e-05, -5.8422e-01, -1.0630e-01,  5.4774e-01,  2.5852e-01,\n",
              "          -7.2705e-02,  2.3929e-02,  5.7376e-01, -1.0784e-01],\n",
              "         [ 3.1893e-02, -5.5883e-01, -9.4389e-02,  4.0751e-01,  4.1343e-01,\n",
              "           1.3550e-01, -2.3127e-02,  6.3960e-01,  3.0715e-03]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increasing Efficiency\n",
        "\n",
        "We now have a successful Multihead Attention layer!! This basically has all the same math and lodgic of attention, except for one small issue: efficiency. Typically we want to avoid for loops as much as possible in our PyTorch code, being able to vectorize and do things in parallel will make much better use of the GPUs we train on. To make this more efficient though, theres something we need to understand first: PyTorch Linear layers on multidimensional tensors!\n",
        "\n",
        "Linear Layers on MultiDimensional Tensors\n",
        "We have already seen nn.Linear(input_dim, output_dim) many times already, and this module expects a tensor of shape [Batch x input_dim] and it will output [Batch x output_dim]. But what if our input is [Batch x Dim1 x Dim2 x input_dim], then what happens? Basically, PyTorch will automatically flatten all the dimensions other than the last one automagically, do the linear layer, and then return back to the expected shape, so we would get an output of [Batch x Dim1 x Dim2 x output_dim]. Another way of thinking about this is, PyTorch linear layers only are applied to the last dimension of your tensor. Lets do a quick example!"
      ],
      "metadata": {
        "id": "VTnwDQ4zEBQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fc = nn.Linear(10,30)\n",
        "\n",
        "tensor_1 = torch.randn(5,10)\n",
        "tensor_1_out = fc(tensor_1)\n",
        "print(\"Input Shape:\", tensor_1.shape, \"Output Shape:\", tensor_1_out.shape)\n",
        "\n",
        "tensor_2 = torch.randn(5,1,2,3,4,10) # multidim tensor, doesnt change the idea, if last dim is = to first dim of fc, it works\n",
        "tensor_2_out = fc(tensor_2)\n",
        "print(\"Input Shape:\", tensor_2.shape, \"Output Shape:\", tensor_2_out.shape)\n",
        "\n",
        "# Input Shape: torch.Size([5, 10]) Output Shape: torch.Size([5, 30])\n",
        "# Input Shape: torch.Size([5, 1, 2, 3, 4, 10]) Output Shape: torch.Size([5, 1, 2, 3, 4, 30])"
      ],
      "metadata": {
        "id": "nBHgejGV81h_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f0e88f0-c6c6-41a0-f11a-104babc47d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape: torch.Size([5, 10]) Output Shape: torch.Size([5, 30])\n",
            "Input Shape: torch.Size([5, 1, 2, 3, 4, 10]) Output Shape: torch.Size([5, 1, 2, 3, 4, 30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Packing Linear Layers\n",
        "\n",
        "Another important idea is packing our linear layers together. Lets think about our example again, each projection for Q, K and V have a Linear layer that takes in 9 values and outputs 3 values, and we repeat this 3 times for each head. Lets just think about our Queries for now.\n",
        "\n",
        "Query for Head 1: Take in input x with embedding dim 9 and outputs tensor with embedding dimension 3\n",
        "\n",
        "Query for Head 2: Take in input x with embedding dim 9 and outputs tensor with embedding dimension 3\n",
        "\n",
        "Query for Head 3: Take in input x with embedding dim 9 and outputs tensor with embedding dimension 3\n",
        "\n",
        "Well what if we reframed this? What if we had a single linear layer that take input x with embedding dim 9 and outputs something with embedding dim 9.\n",
        " Afterwards, we can cut the matrix into our three heads of attention. Lets do a quick example!"
      ],
      "metadata": {
        "id": "qtpYzCIRM_18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tensor = torch.randn(1,8,9)\n",
        "fc = nn.Linear(9,9)\n",
        "\n",
        "### Pass tensor through layer to make Queries ###\n",
        "q = fc(tensor)\n",
        "print(\"Shape of all Queries:\", q.shape)\n",
        "\n",
        "### Cut Embedding dimension into 3 heads ###\n",
        "q_head1, q_head2, q_head3 = torch.chunk(q, 3, axis=-1)\n",
        "print(\"Shape of each Head of Query:\", q_head1.shape)"
      ],
      "metadata": {
        "id": "FFWrdwc281f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aea328f-782c-4aa8-f565-e09ba6c0212c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of all Queries: torch.Size([1, 8, 9])\n",
            "Shape of each Head of Query: torch.Size([1, 8, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Multidimensional mùatrix multiplication\n",
        "\n",
        "a = torch.randn(1,2,6,4) # => will be transofrmed to (1x2,6,4) # 4 as out\n",
        "b = torch.randn(2,1,4,3) #  => will be transofrmed to (1x2,4,3) # 4 as in, so @ is ok\n",
        "print(\"Final Output Shape:\", (a@b).shape) # Final Output Shape: torch.Size([2, 2, 6, 3])\n",
        "\n",
        "\n",
        "#broadcasting des 2 premieres dimensions\n",
        "# Quand tu fais une opération entre deux tenseurs avec des dimensions différentes, PyTorch applique le broadcasting, comme NumPy. Les règles sont :\n",
        "# Si les dimensions sont égales → OK\n",
        "# Si une des dimensions vaut 1 → elle est étendue à l’autre valeur (comme une copie virtuelle)\n",
        "# Sinon → erreur"
      ],
      "metadata": {
        "id": "Jaea5ET581dh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad894c65-910e-4015-fc53-43d2413aeee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Output Shape: torch.Size([2, 2, 6, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Type de Tenseur           \tForme attendue\t            Quand utilisé\n",
        "Entrée (features)\t          [B, S, D]\t                  Entrée du modèle\n",
        "Q, K, V\t                    [B, H, S, D_head]\t          Après projection\n",
        "Scores d’attention          [B, H, S, S]\t              Q @ K^T\n",
        "Attention mask\t            [B, H, S, S]                ou broadcastable à ça\tAppliqué avant softmax\n",
        "Output (features)\t          [B, S, D]\t                  Sortie du modèle\n",
        "\n",
        "# H: num heads\n",
        "# B: batch size\n",
        "# S: Seq_length\n",
        "# D : Embed_dim\n",
        "# D_head = Embed_dim // nul_heads\n",
        "\n",
        "\n",
        "# Important regarding padding !\n",
        "#The model should learn the relation of how the padding tokens are related to the words\n",
        "# (this is most likely just positional information that padding is at the end),\n",
        "# but it should not learn how other words are related to padding,\n",
        "\n",
        "# Function to use for padding computations:\n",
        "# Tensor.masked_fill_ will fill anywhere indicated as True with our fill value.\n",
        "\n",
        "\n",
        "# Self attention vs Cross Attention:\n",
        "# Type d'attention\t        Lignes\t            Colonnes\t          Ex. usage\n",
        "# Self-attention\t          Même séquence\t      Même séquence\t      BERT, GPT\n",
        "# Cross-attention\t          Décodeur (output)\t  Encodeur (input)\t  Traduction, T5, BART"
      ],
      "metadata": {
        "id": "9IVbRCEIxRUP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "55039e5e-6c80-49ba-f994-4acd6c2ec368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid character '’' (U+2019) (<ipython-input-18-a0dee3a15c76>, line 4)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-a0dee3a15c76>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    Scores d’attention          [B, H, S, S]\t              Q @ K^T\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '’' (U+2019)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "More Efficient Attention Implementation\n",
        "\n",
        "We will also include some extra dropout layers typically added to attention computations."
      ],
      "metadata": {
        "id": "rTPawlyGVHJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Self Attention Proposed in `Attention is All  You Need` - https://arxiv.org/abs/1706.03762\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "               embed_dim=768,\n",
        "               num_heads=12,\n",
        "               attn_p=0,\n",
        "               proj_p=0):\n",
        "\n",
        "\n",
        "        # Args:\n",
        "        #     embed_dim: Transformer Embedding Dimension\n",
        "        #     num_heads: Number of heads of computation for Attention\n",
        "        #     attn_p: Probability for Dropout2d on Attention cube\n",
        "        #     proj_p: Probability for Dropout on final Projection\n",
        "\n",
        "\n",
        "        # super(SelfAttentionEncoder, self).__init__()\n",
        "\n",
        "        ### Make Sure Embed Dim is Divisible by Num Heads ###\n",
        "        assert embed_dim % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = int(embed_dim / num_heads)\n",
        "\n",
        "        ### Define all our Projections ###\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim) # matrice de projection Wq(Embed_dim, Embed_dim)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.attn_drop = nn.Dropout(attn_p)\n",
        "\n",
        "        ### Define Post Attention Projection ###\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.proj_drop = nn.Dropout(proj_p)\n",
        "\n",
        "    def forward(self, x): # x.shape = (b,seq_len, embed_dim)\n",
        "\n",
        "        batch, seq_len, embed_dim = x.shape\n",
        "\n",
        "        ### Compute Q, K, V Projections,and Reshape/Permute to [Batch x Num Heads x Seq Len x Head Dim]\n",
        "        q = self.q_proj(x).reshape(batch, seq_len, self.num_heads, self.head_dim).transpose(1,2).contiguous()\n",
        "        # X @ q_proj = X(b, seq_len, embed_dim) @ matrice(Embed_dim,Embed_dim) = Q(b, seq_len, embed_dim)\n",
        "        # donc on peut bien faire le reshape ... derriere\n",
        "        k = self.k_proj(x).reshape(batch, seq_len, self.num_heads, self.head_dim).transpose(1,2).contiguous()\n",
        "        v = self.v_proj(x).reshape(batch, seq_len, self.num_heads, self.head_dim).transpose(1,2).contiguous()\n",
        "\n",
        "        ### Perform Attention Computation ###\n",
        "        attn = (q @ k.transpose(-2,-1)) * (self.head_dim ** -0.5) # k.transpose(-2,-1) => echange seq_len et embed_dim\n",
        "        #q(b, num_heads, seq_len, head_dim) à K^t.transpose(b, num_heads, head_dim, seq_len) = (b, num_heads, seq_len,seq_len)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = attn @ v # (b, num_heads, seq_len,seq_len) @ (b, num_heads, seq_len, head_dim) = (b, num_heads, seq_len, head_dim)\n",
        "\n",
        "        ### Bring Back to [Batch x Seq Len x Embed Dim] ### cause num_heads * head_dim = embed_dim\n",
        "        x = x.transpose(1,2).reshape(batch, seq_len, embed_dim)\n",
        "\n",
        "        ### Pass through Projection so Heads get to know each other ###\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "embed_dim = 9\n",
        "num_heads = 3\n",
        "seq_len = 8\n",
        "a = SelfAttentionEncoder(embed_dim, num_heads)\n",
        "\n",
        "### Create a random tensor in the shape (Batch x Seq Len x Embed Dim) ###\n",
        "rand = torch.randn(3,seq_len,embed_dim)\n",
        "\n",
        "### Pass through MHA ###\n",
        "output = a(rand)\n",
        "print(\"Final Output:\", output.shape)"
      ],
      "metadata": {
        "id": "R1stnnw_81bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attention Mask**\n",
        "\n",
        "Masquage de padding (padding mask) : Quand les séquences d’entrée n’ont pas toutes la même longueur, on les \"pad\" (remplit avec des zéros ou un autre token). Il ne faut pas que le modèle prête attention à ces tokens de padding.\n",
        "\n",
        "Masquage causal (causal mask / autoregressive mask) : Dans des tâches comme la génération de texte, un token à la position i ne doit pas voir les tokens futurs (i+1, i+2, ...). On masque donc les positions futures."
      ],
      "metadata": {
        "id": "eIq8s_8Af0ba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Create an example attention matrix (b x n x n) ###\n",
        "rand_attn = torch.rand(1,6,6)\n",
        "print(\"rand.shape\",rand_attn.shape)\n",
        "\n",
        "# ### Create Attention Mask in the shape (b x n) ###\n",
        "attention_mask = torch.tensor([1,1,1,1,0,0]).unsqueeze(0).bool()\n",
        "print(\"att.shape\",attention_mask.shape)\n",
        "\n",
        "print(\"Method 1:\")\n",
        "print(\"--------\")\n",
        "# ### Add Extra Dimension for the (b x n x n) ###\n",
        "# ### So unsqueeze mask to be (b x 1 x n) ###\n",
        "attention_mask = attention_mask.unsqueeze(1)\n",
        "print(attention_mask.shape)\n",
        "# # ### Unsqueezed with dummy broadcast dimension ###\n",
        "print(attention_mask)\n",
        "print(rand_attn.masked_fill_(~attention_mask, float(\"-inf\")))\n",
        "\n",
        "print(\"Method 2:\")\n",
        "print(\"--------\")\n",
        "### Repeat the Dummy Dimension so attention mask is (b x n x n) ###\n",
        "attention_mask = attention_mask.repeat(1,6,1) # repeat dummy middle dim 6 times (for the seq_len)\n",
        "print(attention_mask)\n",
        "print(rand_attn.masked_fill_(~attention_mask, float(\"-inf\")))"
      ],
      "metadata": {
        "id": "7BOzi_Gaq1m4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b841ddf4-c455-4ffe-e67e-cbfed80999a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rand.shape torch.Size([1, 6, 6])\n",
            "att.shape torch.Size([1, 6])\n",
            "Method 1:\n",
            "--------\n",
            "torch.Size([1, 1, 6])\n",
            "tensor([[[ True,  True,  True,  True, False, False]]])\n",
            "tensor([[[0.0112, 0.2802, 0.7123, 0.2025,   -inf,   -inf],\n",
            "         [0.9080, 0.6380, 0.0561, 0.3039,   -inf,   -inf],\n",
            "         [0.8084, 0.5600, 0.9836, 0.7664,   -inf,   -inf],\n",
            "         [0.8195, 0.6540, 0.8167, 0.9122,   -inf,   -inf],\n",
            "         [0.0505, 0.2526, 0.3384, 0.7304,   -inf,   -inf],\n",
            "         [0.8166, 0.2807, 0.4933, 0.8058,   -inf,   -inf]]])\n",
            "Method 2:\n",
            "--------\n",
            "tensor([[[ True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True, False, False]]])\n",
            "tensor([[[0.0112, 0.2802, 0.7123, 0.2025,   -inf,   -inf],\n",
            "         [0.9080, 0.6380, 0.0561, 0.3039,   -inf,   -inf],\n",
            "         [0.8084, 0.5600, 0.9836, 0.7664,   -inf,   -inf],\n",
            "         [0.8195, 0.6540, 0.8167, 0.9122,   -inf,   -inf],\n",
            "         [0.0505, 0.2526, 0.3384, 0.7304,   -inf,   -inf],\n",
            "         [0.8166, 0.2807, 0.4933, 0.8058,   -inf,   -inf]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0️⃣ Contexte — Nos phrases\n",
        "On a un batch de 2 phrases : (B, seq_len) = (2,4)\n",
        "\n",
        "Batch\tPhrase originale\tTokens (IDs)\n",
        "\n",
        "0\t\"je mange\"\t[12, 45]\n",
        "\n",
        "1\t\"tu bois du café\"\t[33, 67, 90, 123]\n",
        "\n",
        "1️⃣ Padding à longueur max\n",
        "La plus longue phrase a 4 tokens → S = 4.\n",
        "On pad la plus courte à droite :\n",
        "\n",
        "Batch 0 : [12, 45, 0, 0]      # \"je\", \"mange\", PAD, PAD\n",
        "\n",
        "Batch 1 : [33, 67, 90, 123]   # \"tu\", \"bois\", \"du\", \"café\"\n",
        "\n",
        "2️⃣ Mask initial (B, S)\n",
        "\n",
        "Règle : 1 = mot réel, 0 = PAD.\n",
        "\n",
        "\n",
        "attention_mask =\n",
        "tensor(\n",
        "\n",
        "[[1, 1, 0, 0],  \n",
        "[1, 1, 1, 1]])  \n",
        "\n",
        "3️⃣ Après unsqueeze(1).unsqueeze(1) → (B, 1, 1, S)\n",
        "\n",
        "On ajoute deux dimensions au milieu :\n",
        "\n",
        "\n",
        "Batch 0 (\"je mange\"):\n",
        "\n",
        "[[[1, 1, 0, 0]]]\n",
        "\n",
        "Batch 1 (\"tu bois du café\"):\n",
        "\n",
        "[[[1, 1, 1, 1]]]\n",
        "\n",
        "4️⃣ Après repeat(1, 1, S, 1) → (B, 1, S, S)\n",
        "\n",
        "On répète la ligne S fois pour créer la grille (S, S).\n",
        "\n",
        "\n",
        "Batch 0 (\"je mange\"):\n",
        "\n",
        "[[[1, 1, 0, 0],\n",
        "\n",
        "  [1, 1, 0, 0],\n",
        "\n",
        "  [1, 1, 0, 0],\n",
        "\n",
        "  [1, 1, 0, 0]]]\n",
        "\n",
        "Batch 1 (\"tu bois du café\"):\n",
        "\n",
        "[[[1, 1, 1, 1],\n",
        "\n",
        "  [1, 1, 1, 1],\n",
        "\n",
        "  [1, 1, 1, 1],\n",
        "\n",
        "  [1, 1, 1, 1]]]\n",
        "\n",
        "5️⃣ Après ~attention_mask\n",
        "\n",
        "On inverse pour marquer les positions à bloquer :\n",
        "\n",
        "(1 → False, 0 → True)\n",
        "\n",
        "\n",
        "Batch 0 (\"je mange\"):\n",
        "\n",
        "[[[False, False,  True,  True],\n",
        "\n",
        "  [False, False,  True,  True],\n",
        "\n",
        "  [False, False,  True,  True],\n",
        "\n",
        "  [False, False,  True,  True]]]\n",
        "\n",
        "Batch 1 (\"tu bois du café\"):\n",
        "\n",
        "[[[False, False, False, False],\n",
        "\n",
        "  [False, False, False, False],\n",
        "\n",
        "  [False, False, False, False],\n",
        "\n",
        "  [False, False, False, False]]]\n",
        "\n",
        "6️⃣ Après masked_fill(..., -inf)\n",
        "\n",
        "Si attn était rempli de zéros au départ, les positions à True (PAD) deviennent -inf :\n",
        "\n",
        "\n",
        "Batch 0 (\"je mange\"):\n",
        "\n",
        "[[[  0.,   0., -inf, -inf],\n",
        "\n",
        "  [  0.,   0., -inf, -inf],\n",
        "\n",
        "  [  0.,   0., -inf, -inf],\n",
        "\n",
        "  [  0.,   0., -inf, -inf]]]\n",
        "\n",
        "Batch 1 (\"tu bois du café\"):\n",
        "\n",
        "[[[0., 0., 0., 0.],\n",
        "\n",
        "  [0., 0., 0., 0.],\n",
        "\n",
        "  [0., 0., 0., 0.],\n",
        "\n",
        "  [0., 0., 0., 0.]]]\n",
        "\n",
        "📌 À retenir\n",
        "\n",
        "Sans troncature, on garde tous les mots → le mask ne met des 0 que pour les <PAD> ajoutés au remplissage.\n",
        "\n",
        "La duplication (S, S) permet de bloquer toutes les lignes/colonnes vers les positions PAD, peu importe d’où on part dans la séquence.\n",
        "\n"
      ],
      "metadata": {
        "id": "wNGlgRYcAvNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take our previous code and adapt it to add the padding attention masking\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Self Attention Proposed in `Attention is All  You Need` - https://arxiv.org/abs/1706.03762\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 embed_dim=768,\n",
        "                 num_heads=12,\n",
        "                 attn_p=0,\n",
        "                 proj_p=0):\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            embed_dim: Transformer Embedding Dimension\n",
        "            num_heads: Number of heads of computation for Attention\n",
        "            attn_p: Probability for Dropout2d on Attention cube\n",
        "            proj_p: Probability for Dropout on final Projection\n",
        "        \"\"\"\n",
        "\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        ### Make Sure Embed Dim is Divisible by Num Heads ###\n",
        "        assert embed_dim % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = int(embed_dim / num_heads)\n",
        "\n",
        "        ### Define all our Projections ###\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.attn_drop = nn.Dropout(attn_p)\n",
        "\n",
        "        ### Define Post Attention Projection ###\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.proj_drop = nn.Dropout(proj_p)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "\n",
        "        batch, seq_len, embed_dim = x.shape\n",
        "\n",
        "        ### Compute Q, K, V Projections,and Reshape/Permute to [Batch x Num Heads x Seq Len x Head Dim]\n",
        "        q = self.q_proj(x).reshape(batch, seq_len, self.num_heads, self.head_dim).transpose(1,2).contiguous()\n",
        "        k = self.k_proj(x).reshape(batch, seq_len, self.num_heads, self.head_dim).transpose(1,2).contiguous()\n",
        "        v = self.v_proj(x).reshape(batch, seq_len, self.num_heads, self.head_dim).transpose(1,2).contiguous()\n",
        "\n",
        "        ### Perform Attention Computation ###\n",
        "        attn = (q @ k.transpose(-2,-1)) * (self.head_dim ** -0.5)\n",
        "\n",
        "        ####################################################################################\n",
        "        ### FILL ATTENTION MASK WITH -Infinity ###\n",
        "\n",
        "        ### NOTE:\n",
        "        ### attn.shape - (Batch x num_heads x seq_len x seq_len)\n",
        "        ### mask.shape - (Batch x seq_len)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "\n",
        "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(1).repeat(1,1,seq_len,1)\n",
        "            attn = attn.masked_fill(~attention_mask, float('-inf'))# ~ to say \"opposite\"\n",
        "\n",
        "        ####################################################################################\n",
        "\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = attn @ v\n",
        "\n",
        "        ### Bring Back to [Batch x Seq Len x Embed Dim] ###\n",
        "        x = x.transpose(1,2).reshape(batch, seq_len, embed_dim)\n",
        "\n",
        "        ### Pass through Projection so Heads get to know each other ###\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "drqZnP850NDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### We will now have sequences of different lengths, identify the number of tokens in each sequence ###\n",
        "seq_lens = [3,5,4]\n",
        "embed_dim = 9\n",
        "num_heads = 3\n",
        "a = SelfAttention(embed_dim, num_heads)\n",
        "\n",
        "# ### Create a random tensor in the shape (Batch x Seq Len x Embed Dim) ###\n",
        "# ### This will be a tensor upto the max(seq_lens) ###\n",
        "rand = torch.randn(len(seq_lens),max(seq_lens),embed_dim)#\n",
        "rand.shape#torch.Size([3, 5, 9])\n",
        "\n",
        "# ### Create Attention Mask from the seq_lens (shortest sequences padded to the longest ###\n",
        "masks = torch.nn.utils.rnn.pad_sequence([torch.ones(l) for l in seq_lens], batch_first=True, padding_value=0).bool()\n",
        "print(\"Attention Mask:\")\n",
        "print(masks)\n",
        "\n",
        "### Pass through MHA ###\n",
        "output = a(rand, attention_mask=masks)\n",
        "print(\"Final Output:\", output.shape)"
      ],
      "metadata": {
        "id": "wi13exyo0NA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e26fecf3-86cb-4d24-d7fa-aa48889327bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Mask:\n",
            "tensor([[ True,  True,  True, False, False],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True, False]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xnpiXImi0M-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e3f00db-2497-4ea1-f0ef-e0d28885bf4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([1., 1., 1.]), tensor([1., 1., 1., 1., 1.]), tensor([1., 1., 1., 1.])]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets Incorporate the Causal Mask into Self-Attention!\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Self Attention Proposed in `Attention is All  You Need` - https://arxiv.org/abs/1706.03762\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "               embed_dim=768,\n",
        "               num_heads=12,\n",
        "               attn_p=0,\n",
        "               proj_p=0,\n",
        "               causal=False):\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            embed_dim: Transformer Embedding Dimension\n",
        "            num_heads: Number of heads of computation for Attention\n",
        "            attn_p: Probability for Dropout2d on Attention cube\n",
        "            proj_p: Probability for Dropout on final Projection\n",
        "            causal: Do you want to apply a causal mask?\n",
        "        \"\"\"\n",
        "\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        ### Make Sure Embed Dim is Divisible by Num Heads ###\n",
        "        assert embed_dim % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = int(embed_dim / num_heads)\n",
        "        self.causal = causal\n",
        "\n",
        "        ### Define all our Projections ###\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.attn_drop = nn.Dropout(attn_p)\n",
        "\n",
        "        ### Define Post Attention Projection ###\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.proj_drop = nn.Dropout(proj_p)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "\n",
        "        batch, seq_len, embed_dim = x.shape\n",
        "\n",
        "        ### Compute Q, K, V Projections,and Reshape/Permute to [Batch x Num Heads x Seq Len x Head Dim]\n",
        "        q = self.q_proj(x).reshape(batch, seq_len, self.num_heads, self.head_dim).transpose(1,2).contiguous()\n",
        "        k = self.k_proj(x).reshape(batch, seq_len, self.num_heads, self.head_dim).transpose(1,2).contiguous()\n",
        "        v = self.v_proj(x).reshape(batch, seq_len, self.num_heads, self.head_dim).transpose(1,2).contiguous()\n",
        "\n",
        "        ### Perform Attention Computation ###\n",
        "        attn = (q @ k.transpose(-2,-1)) * (self.head_dim ** -0.5)\n",
        "\n",
        "        if self.causal:\n",
        "            ####################################################################################\n",
        "            ### Create the Causal Mask (On the correct device) ###\n",
        "\n",
        "            ### Create a Seq_Len x Seq_Len tensor full of Ones\n",
        "            ones = torch.ones((seq_len, seq_len), device=attn.device)\n",
        "\n",
        "            ### Fill Top right triangle with Zeros (as we dont want to attend to them) ###\n",
        "            causal_mask = torch.tril(ones)\n",
        "\n",
        "            ### Add extra dimensions for Batch size and Number of Heads ###\n",
        "            causal_mask = causal_mask.reshape(1,1,seq_len,seq_len).bool()\n",
        "\n",
        "            ### If we have padding mask, then update our causal mask ###\n",
        "            if attention_mask is not None:\n",
        "\n",
        "                ### Each sample could have a different number of pad tokens, so repeat causal mask for batch size ###\n",
        "                causal_mask = causal_mask.repeat(batch, 1, 1, 1)\n",
        "\n",
        "                ### Expand and repeat the Padding Mask (b x s) -> (b x 1 x s x s)###\n",
        "                attention_mask = attention_mask.unsqueeze(1).unsqueeze(1).repeat(1,1,seq_len,1)\n",
        "\n",
        "                ### Fill causal mask where attention mask is False with False (to ensure all padding tokens are masked out) ###\n",
        "                causal_mask = causal_mask.masked_fill(~attention_mask, False)\n",
        "\n",
        "            ### Fill attn with -inf wherever causal mask is False ###\n",
        "            attn = attn.masked_fill(~causal_mask, float('-inf'))\n",
        "\n",
        "        ####################################################################################\n",
        "\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = attn @ v\n",
        "\n",
        "        ### Bring Back to [Batch x Seq Len x Embed Dim] ###\n",
        "        x = x.transpose(1,2).reshape(batch, seq_len, embed_dim)\n",
        "\n",
        "        ### Pass through Projection so Heads get to know each other ###\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "### We will now have sequences of different lengths, identify the number of tokens in each sequence ###\n",
        "seq_lens = [3,5,4]\n",
        "embed_dim = 9\n",
        "num_heads = 3\n",
        "a = SelfAttention(embed_dim, num_heads, causal=True)\n",
        "\n",
        "### Create a random tensor in the shape (Batch x Seq Len x Embed Dim) ###\n",
        "### This will be a tensor upto the max(seq_lens) ###\n",
        "rand = torch.randn(len(seq_lens),max(seq_lens),embed_dim)\n",
        "\n",
        "### Create Attention Mask from the seq_lens (shortest sequences padded to the longest ###\n",
        "masks = torch.nn.utils.rnn.pad_sequence([torch.ones(l) for l in seq_lens], batch_first=True, padding_value=0).bool()\n",
        "print(\"Attention Mask:\")\n",
        "print(masks)\n",
        "\n",
        "### Pass through MHA ###\n",
        "output = a(rand, attention_mask=masks)\n",
        "print(\"Final Output:\", output.shape)"
      ],
      "metadata": {
        "id": "LIpmkk8v0M4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RFi_QSXX0MzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Create a Seq_Len x Seq_Len tensor full of Ones\n",
        "seq_len = 9\n",
        "ones = torch.ones((seq_len, seq_len))\n",
        "ones\n",
        "### Fill Top right triangle with Zeros (as we dont want to attend to them) ###\n",
        "causal_mask = torch.tril(ones)\n",
        "\n",
        "# ### Add extra dimensions for Batch size and Number of Heads ###\n",
        "causal_mask = causal_mask.reshape(1,1,seq_len,seq_len).bool()\n",
        "causal_mask"
      ],
      "metadata": {
        "id": "CKN4K7OB0Mwk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf8c3ff1-16e4-42d6-af93-e74c8c45bdf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ True, False, False, False, False, False, False, False, False],\n",
              "          [ True,  True, False, False, False, False, False, False, False],\n",
              "          [ True,  True,  True, False, False, False, False, False, False],\n",
              "          [ True,  True,  True,  True, False, False, False, False, False],\n",
              "          [ True,  True,  True,  True,  True, False, False, False, False],\n",
              "          [ True,  True,  True,  True,  True,  True, False, False, False],\n",
              "          [ True,  True,  True,  True,  True,  True,  True, False, False],\n",
              "          [ True,  True,  True,  True,  True,  True,  True,  True, False],\n",
              "          [ True,  True,  True,  True,  True,  True,  True,  True,  True]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D73JZOMO0Mr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gzhV0Xch0MpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xS5kJHfO0Mmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, S, D = x.shape\n",
        "        qkv = self.qkv_proj(x)  # shape: [B, S, 3D]\n",
        "        q, k, v = qkv.chunk(3, dim=-1)  # each: [B, S, D]\n",
        "\n",
        "        attn_scores = Q @ K.transpose(1,2) * (3embed_dim ** 0.5)  #\n",
        "        attn_weights = attn_scores.softmax(dim = -1)  #\n",
        "        context =    attn_weights @ V    #\n",
        "        out = self.out_proj(context)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "3skm6K3U0Mhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "mask = torch.tensor([[1, 1, 1, 0],\n",
        "                     [1, 1, 0, 0]]).bool()  # shape [B, S] = [2, 4]\n",
        "scores = torch.randn(2, 4, 4)  # [B, Q_len, K_len] — donc ici [2, 4, 4]\n",
        "\n",
        "# Étape 1 : rendre le mask compatible avec scores [B, Q, K]\n",
        "mask = mask.unsqueeze(1)  # Devient [B, 1, K]\n",
        "\n",
        "# Étape 2 : appliquer le masque\n",
        "masked_scores = scores.masked_fill(mask, float('-inf'))\n",
        "\n",
        "# Étape 3 : softmax\n",
        "attn_weights = torch.softmax(masked_scores, dim=-1)\n"
      ],
      "metadata": {
        "id": "Y5pM1hk9Mmic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "seq_len = 4\n",
        "causal_mask = torch.tril(torch.ones(seq_len,seq_len)).bool()\n"
      ],
      "metadata": {
        "id": "MfCXhvGvMmfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0NL_NyMyMmdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MUKdJ1iUMmaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e9GVIC5JMmXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YFZN64eNMmTm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}